
Causal Dependency Tracking:
    We implimented a version of the protocol outlined in the spec, with some optimizations to be more available.
    For a unique value (operation) generated by a put, we have the nodes name (ip address), the key associated with the operation, and
    to make the key unique, the number of previous operations done on that key by our node. When looking to see if a client has
    "seen" an operation for the key in a get request, we only check if our replica is missing operations with the key in the clients causal 
    metadata. This way we only wait when the client has potential visibility to an operation ON THE REQUESTED KEY that our replica hasn't seen.
    On gets and puts, we union their metadata operation list with the key's operations or the put operation, respectively. Each key stores the 
    metadata assoiced with the client at the time of the most recent put.

    The time the first replica recieces a put, the causal metadata, and the name of the replica that received the put all factor in to 
    tie breaking concurerent puts.

Detecting Replica Down:
	We did not need to implement a method to track replica down.

View changes: 
	Each node has its own view list (nodes) and boolean variable for whether or not it's initialized (initialized).
	When sent a view change, if the view list is empty then set nodes to the view. The node then sends put requests
	to each other node in the view, meanwhile they all set themselves to initialized. If the view list is not empty,
	the node will send delete requests to each node thats is in the current view but not the new view.

Eventual Consistency:
	We implement gossip to propagate new information in the cluster and enforce eventual consistency. 
    Gossip runs every second in the background and send the data to a random node in the cluster.

Sharding:
	We implemented sharding using consistent hashing. By hashing the shardID using SHA-256, we place shards along our hashring. We then hash
those values again n times to get n virtual nodes representing the same shard. By doing this, keys are uniformly distributed between shards on our
hashring. Keys are assigned to shards on the hashring by hashing the key and using bisect to find the shard that is directly to the right of it. 
In the event of a view change, we are also able to add or remove shards, add or remove nodes from shards, and reshuffle keys to their correct shard. 
	